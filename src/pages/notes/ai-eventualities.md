---
pubDate: April 1, 2024
title: "The Eventualities of AI"
summary: My predictions on the future of creative generation
tags: productivity
layout: ../../layouts/Blog.astro
---

I will make some predictions. This site gets indexed by Archive.org occasionally, so I'll have a hard time scrubbing the internet of this record once I publish. Hello future people! Feel free to bask in my genius or laugh off the rest of this article.

The prediction "X will never Y" is often stated without considering whether "Y" is a categorical limitation or just something not currently supported. Phones at one point did not bear touch screens. Internet was wired and loud. These were ephemeral limitations in a medium - not something everlasting.

I remember only a few months ago people laughing off tools like Copilot or ChatGPT because of its mistakes as if these were inherent shortcomings of the technology. Did it make these semantic errors because the concept of an LLM is inherently incompatible with Objective C or does your LLM just lack training data?

AI shape tracking, object identification, media generation, and tooling were all both poor and expensive in recent time. This has changed. They have improved and will continue to improve.

To the average geek, this looks like we sit at the precipice of generalized artificial intelligence. Look a bit closer. We are in no such space. A good understanding of how these tools work shows you that we've crafted algorithms that simulate cellular automata that approximate biological systems, yet come nowhere close in performance or generalizability. 

LLMs and GANs produce content that is remarkably similar to human output, but their understanding is so disparate from our own that it makes control and prediction a challenge we'll never fully beat. We can use GANs and LLMs to produce content that fools GANs and LLMs, but not another human. That is not to say that GANs and LLMs cannot outperform and fool humans, but that there is a fundamental difference in content creation.

With this preface, here are my predictions.
# We Will Strengthen Art

The current state of AI tooling will be helpful in content creation. The first to go will be (and has been) boring creative jobs. This removes stock media used in articles, commercials, background work in larger creative pieces, jingles, billboards, etc. 

A larger downside of this process is that this is generally where a significant portion of artists' income originates. This will hurt artists, but it will not hurt art as much. Yes, there is a cascade - lower pay for artists means fewer artists doing art. I just don't think the effect will be as extreme.

We will see a concentration and re-realization of art. We are now seeing people place this "art" in places reserved for actual works of human expression. This is a mistake and I don't think it will hold for longer. At some point, media generation becomes so simple that the art becomes worthless and we will turn our focus to actual creative efforts.

Sure, you can generate another Mona Lisa, but I can also screenshot it. The novelty of a new Da Vinci will be overshadowed by its lack of humanity or taste. We will start to realize that real art is not defined by its photo-realism or shapes, but instead by the being that designed it and our interpretations as an extension of it.

In music and art, many people seem to feel its elegance as much as it tickles their optic or acoustic sensibilities. It sounds good and looks nice - therefore it is art. Once it becomes easy to generate visually interesting media and new pop beats, you will simply do that. You will create the media you want to see and then realize there is no art here.

This awakening will trigger a larger focus on the art portion of art. Media will no longer gain significance for its sparkle, but instead its beauty.

# We Will Strengthen Authentication

I see fear-mongering over "fake news" and media manipulation. In some way, this is a blessing. We are taking a problem already present and placing it under a greater lens. 

The way this is solved is **not** by controlling the means of generation. It's just not practical. This seems to scare people. I get it. I only suggest thinking about this from the opposite direction.

The onus of authenticity can instead be on good actors. This feels unfair, but when automated it is a non-issue. If your programs and devices authenticate the media they produce, you can have a verifiable record of origin. This gives new media a harder time faking its origin since these systems will be cryptographically based. 

This system only works when everything is authenticated. Unfortunately, most content has yet to be verified. We'll probably find a fix for that at some point (somehow verifying all images before some date). Going forward, we will likely have all content signed in this manner.

The path will follow the "HTTPS pattern". At some point, dear reader (like a few years ago), we did not expect websites to use SSL/TLS. If a website supported this new-age encryption standard, we marked it with "https" and gave it this cool green lock to show how awesome it was. Eventually, this green lock became so common that the lack of its presence was now novel. 

It was not "normal sites" and "secure sites" - it was "normal sites" and "insecure sites". You get a "Not Secure" warning these days for HTTP websites. It is the "not verified" warning I believe we will soon come to expect plastered on all our media.

If something has not been authenticated, it will be strange and worthy of warning. This is how we will solve the fake news.

# We Will Strengthen Regulation

This is already happening. A large issue with putting "AI" in control of systems is the aforementioned problem of reasoning about a non-human system. There is an additional problem with blame.

Without blame, we lose incentive structures. We need to blame some **thing** and that thing needs to circle back to reinforce beneficial incentive structures. Placing blame squarely on an algorithm creates a very dangerous environment.

Luckily this has already shown its head in a few instances. Chatbots with total control are reflecting poorly on large businesses. If you are not vetting the output of a system that does not abide by human reason, you're in for a bad time. And good. You will learn your lesson.

We will make a few missteps in regulation. Regulating the use of math never works out too well. Our regulation should not be around the ownership of software or the security research around dangerous systems. It will be, but we will learn. These are not practical and will soon be eclipsed by the portability and ready access of these tools.

What we do with these systems - how we use content they produce. This will be the subject of scrutiny. Publishing misleading content. Fraud. Libel. We already have systems for these, but we will strengthen our laws around AI-generated content to greater discourage its harmful use.

As law tends to be a push and pull, we won't ever get it right. But we will try.

# We Will Refocus

These tools have tremendous productive potential. With that comes the elimination of hundreds of thousands of jobs. It also means the creation of new industries, but it won't replace those we have displaced. 

This will allow us to produce with even greater efficiency. It is another multiplier. But just as great are the hazards.

With each leap in "wealth," there comes a time to refocus our energy on leveling the quality of living. I don't know if we will be successful, but there will be effort. There are jobs that need to be done. There are jobs yet to be created. There will be what to do. There is a path forward.

Our success. I am afraid to guess on this.

\# end note